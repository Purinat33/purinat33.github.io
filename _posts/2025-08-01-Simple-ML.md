---
title: "Student Performance Prediction"
categories: [Artificial Intelligence and Machine Learning, Data Analytics]
toc: true
# Optional but recommended for clean image paths:
media_subpath: /assets/img/student-performance
---

# Student Performance Prediction (Regression)

This project predicts a student’s **final grade (G3)** using demographic, social, and school-related features from the UCI Student Performance dataset.

- Dataset: https://archive.ics.uci.edu/dataset/320/student+performance

> Goal: build a model that predicts **G3** as a numeric value (regression), then compare models using **R²** and **MAE**.

---

## Problem statement

Given student attributes (e.g., study time, family support, absences, etc.) and prior grades (**G1, G2**), predict the **final grade G3**.

- **Task**: Regression (predict G3 directly)
- **Why not only accuracy**: For regression, we evaluate error size using MAE and model fit using R².

---

## Data overview

- The source CSV is **semicolon-separated**.
- No missing values were observed during inspection.
- Target:
  - `y = G3` (final grade)
- Features:
  - `X = all columns except G3` (including `G1` and `G2`)

> Practical note: You can load the file directly with `pd.read_csv(..., sep=';')` instead of rewriting the CSV.

---

## Exploratory analysis

### Categorical value sanity checks

To ensure categorical columns had no typos/inconsistent labels, I checked distributions (including splits by gender) and saved a combined plot.

![Column distributions](/graph.png){: .shadow w="900" }

### Low-dimensional visualization (PCA / t-SNE)

I tried PCA and t-SNE to see whether G3 separates cleanly in 2D space. The plots showed heavy overlap between scores, suggesting that strong performance comes from **higher-dimensional feature interactions**, not a simple 2D separation.

![PCA plot](/pca.png){: .shadow w="800" }
![t-SNE plot](/tsne.png){: .shadow w="800" }

---

## Preprocessing

### Train/test split

- 70% train / 30% test
- Shuffle with fixed random seed for reproducibility

### Encoding + scaling

- **Categorical features**: OneHotEncoder (`handle_unknown="ignore"`)
- **Numeric real-valued features**: StandardScaler (with `with_mean=False` due to sparse output from one-hot)
- **Binary numeric features**: passed through

This was wrapped into a `ColumnTransformer`, then later into a full `Pipeline`.

---

## Modeling approach

### Metrics

- **R²**: higher is better (closer to 1)
- **MAE**: lower is better (average absolute error in grade points)

> MAE is the easiest metric to explain: “we’re off by ~1.1 points on average.”

### Baseline

A `DummyRegressor(strategy="mean")` provides a sanity-check baseline.

## Model comparison (cross-validation)

I compared multiple regression models using cross-validation and tracked both **R²** (higher is better) and **MAE** (lower is better).

| Model                      |        R² | MAE (grade points) |
| -------------------------- | --------: | -----------------: |
| Linear Regression          |     0.814 |              1.381 |
| Lasso (L1)                 |     0.824 |              1.070 |
| Ridge (L2)                 |     0.816 |              1.365 |
| Linear Regression (scaled) |     0.814 |              1.375 |
| SVR                        |     0.556 |              1.941 |
| Decision Tree              |     0.726 |              1.221 |
| KNN                        |     0.608 |              2.034 |
| Random Forest              |     0.858 |          **0.999** |
| Gradient Boosting          | **0.874** |              1.014 |

**Pick:** I selected **Random Forest** as the primary model because it achieved the **lowest MAE (~1.0 grade point)** while still maintaining strong R².

> Note: Gradient Boosting had the best R², but Random Forest had slightly better MAE, which I prioritized for “how many points off” interpretability.

---

## Tuning and validation

### Feature selection (percentile)

I tried dropping less informative features using `SelectPercentile(f_regression)`. This produced only a **minor improvement**, suggesting the model already handles feature interactions well.

### Hyperparameter tuning (GridSearchCV)

I tuned parameters such as:

- `max_depth`
- `n_estimators`

The improvement was small, but GridSearch helped confirm a stable configuration and avoided choosing parameters arbitrarily.

---

## Final performance (summary)

From the final pipeline + test evaluation:

- **Final Test R²**: ~0.8
- **Final Test MAE**: ~1.1

Interpretation:

- The model predicts final grades reasonably well, with average error around **1 grade point**.

> One important caveat: including `G1` and `G2` makes the task easier and can inflate performance compared to predicting from only demographic/social features.

---

## Bonus: classification version

As an extra experiment, I binned G3 into grade bands (A/B/C/F) and trained a classifier (RandomForestClassifier). This version is useful when the goal is “grade category” rather than exact score.

---

## What I learned

- Data preparation and preprocessing mattered more than model selection.
- Encoding and scaling choices strongly affected linear models.
- PCA/t-SNE did not reveal clear separation, but the final model still performed well—evidence that the relationship is not reducible to simple 2D structure.

---

## Key code excerpt (pipeline + grid search)

```python
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, KFold, cross_val_score

X = df.iloc[:, 0:-1]
y = df.iloc[:, -1]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, shuffle=True, random_state=42
)

pre = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols_raw),
        ("num_real", StandardScaler(with_mean=False), num_real),
        ("num_bin", "passthrough", num_binary),
    ],
    remainder="drop"
)

pipe = Pipeline([
    ("transformer", pre),
    ("model", RandomForestRegressor(random_state=42))
])

param_grid = {
    "model__max_depth": [2, 3, 4, 5, 6, 7],
    "model__n_estimators": [50, 100, 200, 300],
}

grid = GridSearchCV(pipe, param_grid=param_grid)
grid.fit(X_train, y_train)
```

---

## References

- Cortez, P. (2008). Student Performance [Dataset]. UCI Machine Learning Repository. [https://doi.org/10.24432/C5TG7T](https://doi.org/10.24432/C5TG7T)

---
